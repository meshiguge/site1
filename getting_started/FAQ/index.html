<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="BigMoyan">
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>FAQ - Keras中文文档</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../../css/highlight.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "FAQ";
    var mkdocs_page_input_path = "getting_started/FAQ.md";
    var mkdocs_page_url = "/getting_started/FAQ/";
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js"></script>
  <script src="../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../js/highlight.pack.js"></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> Keras中文文档</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../..">主页</a>
        
    </li>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>快速开始</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../concepts/">一些基本概念</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../keras_linux/">Keras安装和配置指南(Linux)</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../keras_windows/">Keras安装和配置指南(Windows)</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../sequential_model/">快速开始Sequential模型</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../functional_API/">快速开始泛型模型</a>
        
    </li>

        
            
    <li class="toctree-l1 current">
        <a class="current" href="./">FAQ</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#keras-faq">Keras FAQ：常见问题</a></li>
                
                    <li><a class="toctree-l4" href="#keras">如何引用Keras？</a></li>
                
                    <li><a class="toctree-l4" href="#kerasgpu">如何使Keras调用GPU？</a></li>
                
                    <li><a class="toctree-l4" href="#keras_1">如何保存Keras模型？</a></li>
                
                    <li><a class="toctree-l4" href="#_1">为什么训练误差比测试误差高很多？</a></li>
                
                    <li><a class="toctree-l4" href="#_2">如何获取中间层的输出？</a></li>
                
                    <li><a class="toctree-l4" href="#keras_2">如何利用Keras处理超过机器内存的数据集？</a></li>
                
                    <li><a class="toctree-l4" href="#loss">当验证集的loss不再下降时，如何中断训练？</a></li>
                
                    <li><a class="toctree-l4" href="#_3">验证集是如何从训练集中分割出来的？</a></li>
                
                    <li><a class="toctree-l4" href="#_4">训练数据在训练时会被随机洗乱吗？</a></li>
                
                    <li><a class="toctree-l4" href="#epochloss">如何在每个epoch后记录训练/测试的loss和正确率？</a></li>
                
                    <li><a class="toctree-l4" href="#rnnstatful-rnn">如何使用状态RNN（statful RNN）？</a></li>
                
                    <li><a class="toctree-l4" href="#kerasgpu_1">如何使用Keras进行分布式/多GPU运算？</a></li>
                
                    <li><a class="toctree-l4" href="#_5">如何“冻结”网络的层？</a></li>
                
                    <li><a class="toctree-l4" href="#sequential">如何从Sequential模型中去除一个层？</a></li>
                
                    <li><a class="toctree-l4" href="#keras_3">如何在Keras中使用预训练的模型？</a></li>
                
            
            </ul>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../trap/">Keras使用陷阱</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../examples/">Keras示例列表</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>模型</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../models/about_model/">关于Keras模型</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../models/sequential/">Sequential模型</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../models/model/">泛型模型</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>网络层</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../layers/about_layer/">关于Keras层</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../layers/core_layer/">常用层Core</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../layers/convolutional_layer/">卷积层Convolutional</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../layers/pooling_layer/">池化层Pooling</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../layers/locally_connected_layer/">局部连接层Locally-connented</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../layers/recurrent_layer/">递归层Recurrent</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../layers/embedding_layer/">嵌入层Embedding</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../layers/advanced_activation_layer/">高级激活层Advanced Activation</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../layers/normalization_layer/">规范层BatchNormalization</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../layers/noise_layer/">噪声层Noise</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../layers/wrapper/">包装器Wrapper</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../layers/writting_layer/">编写自己的层</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>预处理</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../preprocessing/sequence/">序列预处理</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../preprocessing/text/">文本预处理</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../preprocessing/image/">图片预处理</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>其他重要模块</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../other/objectives/">目标函数Objective</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../other/optimizers/">优化器Optimizer</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../other/activations/">激活函数Activation</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../other/callbacks/">回调函数Callback</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../other/metrices/">性能评估Metrices</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../other/initializations/">初始化方法Initialization</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../other/regularizers/">正则项Regularizer</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../other/constraints/">约束项Constraint</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../other/application/">预训练模型Application</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../other/datasets/">常用数据库Dataset</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../other/visualization/">可视化Visualization</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../backend/">keras后端Backend</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../scikit-learn_API/">scikit-learn接口</a>
        
    </li>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>工具</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../utils/data_utils/">数据工具</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../utils/io_utils/">输入输出I/O</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../utils/layer_utils/">Keras层工具</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../utils/np_utils/">numpy工具</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>深度学习与Keras</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../blog/cnn_see_world/">CNN眼中的世界</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../blog/autoencoder/">花式自动编码器</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../blog/image_classification_using_very_little_data/">面向小数据集构建图像分类模型</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../blog/word_embedding/">在Keras模型中使用预训练的词向量</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../blog/keras_and_tensorflow/">将Keras作为tensorflow的精简接口</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../acknowledgement/">致谢</a>
        
    </li>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">Keras中文文档</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>快速开始 &raquo;</li>
        
      
    
    <li>FAQ</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="keras-faq">Keras FAQ：常见问题</h1>
<ul>
<li><a href="#citation">如何引用Keras？</a></li>
<li><a href="#GPU">如何使Keras调用GPU？</a></li>
<li><a href="#save_model">如何保存Keras模型？</a></li>
<li><a href="#loss">为什么训练误差(loss)比测试误差高很多？</a></li>
<li><a href="#intermediate_layer">如何获取中间层的输出？</a></li>
<li><a href="#dataset">如何利用Keras处理超过机器内存的数据集？</a></li>
<li><a href="#stop_train">当验证集的loss不再下降时，如何中断训练？</a></li>
<li><a href="#validation_spilt">验证集是如何从训练集中分割出来的？</a></li>
<li><a href="#shuffle">训练数据在训练时会被随机洗乱吗？</a></li>
<li><a href="#history">如何在每个epoch后记录训练/测试的loss和正确率？</a></li>
<li><a href="#statful_RNN">如何使用状态RNN（statful RNN）？</a></li>
<li><a href="#multi-GPU">如何使用Keras进行分布式/多GPU运算？</a></li>
<li><a href="#freeze">如何“冻结”网络的层？</a></li>
<li><a href="#pop">如何从Sequential模型中去除一个层？</a></li>
<li><a href="#pretrain">如何在Keras中使用预训练的模型</a></li>
</ul>
<hr />
<p><a name='citation'>
<font color='#404040'></p>
<h2 id="keras">如何引用Keras？</h2>
<p></font>
</a></p>
<p>如果Keras对你的研究有帮助的话，请在你的文章中引用Keras。这里是一个使用BibTex的例子</p>
<pre><code class="python">@misc{chollet2015keras,
  author = {Chollet, François},
  title = {Keras},
  year = {2015},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/fchollet/keras}}
}
</code></pre>

<hr />
<p><a name='GPU'>
<font color='#404040'></p>
<h2 id="kerasgpu">如何使Keras调用GPU？</h2>
<p></font>
</a></p>
<p>如果采用TensorFlow作为后端，当机器上有可用的GPU时，代码会自动调用GPU进行并行计算。如果使用Theano作为后端，可以通过以下方法设置：</p>
<p>方法1：使用Theano标记</p>
<p>在执行python脚本时使用下面的命令：</p>
<pre><code class="python">THEANO_FLAGS=device=gpu,floatX=float32 python my_keras_script.py
</code></pre>

<p>方法2：设置<code>.theano</code>文件</p>
<p><font color='#404040'>点击<a href="http://deeplearning.net/software/theano/library/config.html"><font color="FF0000">这里</font></a>查看指导教程</p>
<p>方法3：在代码的开头处手动设置<code>theano.config.device</code>和<code>theano.config.floatX</code></p>
<pre><code class="python">    import theano
    theano.config.device = 'gpu'
    theano.config.floatX = 'float32'
</code></pre>

<hr />
<p><a name='save_model'>
<font color='#404040'></p>
<h2 id="keras_1">如何保存Keras模型？</h2>
<p></font>
</a></p>
<p>我们不推荐使用pickle或cPickle来保存Keras模型</p>
<p>你可以使用<code>model.save(filepath)</code>将Keras模型和权重保存在一个HDF5文件中，该文件将包含：</p>
<ul>
<li>模型的结构，以便重构该模型</li>
<li>模型的权重</li>
<li>训练配置（损失函数，优化器等）</li>
<li>优化器的状态，以便于从上次训练中断的地方开始</li>
</ul>
<p>使用<code>keras.models.load_model(filepath)</code>来重新实例化你的模型，如果文件中存储了训练配置的话，该函数还会同时完成模型的编译</p>
<p>例子：</p>
<pre><code class="python">from keras.models import load_model

model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'
del model  # deletes the existing model

# returns a compiled model
# identical to the previous one
model = load_model('my_model.h5')
</code></pre>

<p>如果你只是希望保存模型的结构，而不包含其权重或配置信息，可以使用：</p>
<pre><code class="python"># save as JSON
json_string = model.to_json()

# save as YAML
yaml_string = model.to_yaml()
</code></pre>

<p>这项操作将把模型序列化为json或yaml文件，这些文件对人而言也是友好的，如果需要的话你甚至可以手动打开这些文件并进行编辑。</p>
<p>当然，你也可以从保存好的json文件或yaml文件中载入模型：</p>
<pre><code class="python"># model reconstruction from JSON:
from keras.models import model_from_json
model = model_from_json(json_string)

# model reconstruction from YAML
model = model_from_yaml(yaml_string)
</code></pre>

<p>如果需要保存模型的权重，可通过下面的代码利用HDF5进行保存。注意，在使用前需要确保你已安装了HDF5和其Python库h5py</p>
<pre><code>model.save_weights('my_model_weights.h5')
</code></pre>

<p>如果你需要在代码中初始化一个完全相同的模型，请使用：</p>
<pre><code class="python">model.load_weights('my_model_weights.h5')
</code></pre>

<p>如果你需要加载权重到不同的网络结构（有些层一样）中，例如fine-tune或transfer-learning，你可以通过层名字来加载模型：</p>
<pre><code class="python">model.load_weights('my_model_weights.h5', by_name=True)
</code></pre>

<p>例如：</p>
<pre><code class="python">&quot;&quot;&quot;
假如原模型为：
    model = Sequential()
    model.add(Dense(2, input_dim=3, name=&quot;dense_1&quot;))
    model.add(Dense(3, name=&quot;dense_2&quot;))
    ...
    model.save_weights(fname)
&quot;&quot;&quot;
# new model
model = Sequential()
model.add(Dense(2, input_dim=3, name=&quot;dense_1&quot;))  # will be loaded
model.add(Dense(10, name=&quot;new_dense&quot;))  # will not be loaded

# load weights from first model; will only affect the first layer, dense_1.
model.load_weights(fname, by_name=True)

</code></pre>

<hr />
<p><a name='loss'>
<font color='#404040'></p>
<h2 id="_1">为什么训练误差比测试误差高很多？</h2>
<p></font>
</a></p>
<p>一个Keras的模型有两个模式：训练模式和测试模式。一些正则机制，如Dropout，L1/L2正则项在测试模式下将不被启用。</p>
<p>另外，训练误差是训练数据每个batch的误差的平均。在训练过程中，每个epoch起始时的batch的误差要大一些，而后面的batch的误差要小一些。另一方面，每个epoch结束时计算的测试误差是由模型在epoch结束时的状态决定的，这时候的网络将产生较小的误差。</p>
<p>【Tips】可以通过定义回调函数将每个epoch的训练误差和测试误差并作图，如果训练误差曲线和测试误差曲线之间有很大的空隙，说明你的模型可能有过拟合的问题。当然，这个问题与Keras无关。【@BigMoyan】</p>
<hr />
<p><a name='intermediate_layer'>
<font color='#404040'></p>
<h2 id="_2">如何获取中间层的输出？</h2>
<p></font>
</a></p>
<p>一种简单的方法是创建一个新的<code>Model</code>，使得它的输出是你想要的那个输出</p>
<pre><code class="python">from keras.models import Model

model = ...  # create the original model

layer_name = 'my_layer'
intermediate_layer_model = Model(input=model.input,
                                 output=model.get_layer(layer_name).output)
intermediate_output = intermediate_layer_model.predict(data
</code></pre>

<p>此外，我们也可以建立一个Keras的函数来达到这一目的：</p>
<pre><code class="python">from keras import backend as K

# with a Sequential model
get_3rd_layer_output = K.function([model.layers[0].input],
                                  [model.layers[3].output])
layer_output = get_3rd_layer_output([X])[0]
</code></pre>

<p>当然，我们也可以直接编写Theano和TensorFlow的函数来完成这件事</p>
<p>注意，如果你的模型在训练和测试两种模式下不完全一致，例如你的模型中含有Dropout层，批规范化（BatchNormalization）层等组件，你需要在函数中传递一个learning_phase的标记，像这样：</p>
<pre><code>get_3rd_layer_output = K.function([model.layers[0].input, K.learning_phase()],
                                  [model.layers[3].output])

# output in test mode = 0
layer_output = get_3rd_layer_output([X, 0])[0]

# output in train mode = 1
layer_output = get_3rd_layer_output([X, 1])[0]
</code></pre>

<hr />
<p><a name='dataset'>
<font color='#404040'></p>
<h2 id="keras_2">如何利用Keras处理超过机器内存的数据集？</h2>
<p></font>
</a></p>
<p>可以使用<code>model.train_on_batch(X,y)</code>和<code>model.test_on_batch(X,y)</code>。请参考<a href="../../models/sequential/"><font color='#FF0000'>模型</font></a></p>
<p>另外，也可以编写一个每次产生一个batch样本的生成器函数，并调用<code>model.fit_generator(data_generator, samples_per_epoch, nb_epoch)</code>进行训练</p>
<p>这种方式在Keras代码包的example文件夹下CIFAR10例子里有示范，也可点击<a href="https://github.com/fchollet/keras/blob/master/examples/cifar10_cnn.py"><font color='#FF0000'>这里</font></a>在github上浏览。</p>
<hr />
<p><a name='early_stopping'>
<font color='#404040'></p>
<h2 id="loss">当验证集的loss不再下降时，如何中断训练？</h2>
<p></font>
</a></p>
<p>可以定义<code>EarlyStopping</code>来提前终止训练</p>
<pre><code class="python">from keras.callbacks import EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss', patience=2)
model.fit(X, y, validation_split=0.2, callbacks=[early_stopping])
</code></pre>

<p>请参考<a href="../../other/callbacks"><font color='#FF0000'>回调函数</font></a></p>
<hr />
<p><a name='validation_spilt'>
<font color='#404040'></p>
<h2 id="_3">验证集是如何从训练集中分割出来的？</h2>
<p></font>
</a></p>
<p>如果在<code>model.fit</code>中设置<code>validation_spilt</code>的值，则可将数据分为训练集和验证集，例如，设置该值为0.1，则训练集的最后10%数据将作为验证集，设置其他数字同理。注意，原数据在进行验证集分割前并没有被shuffle，所以这里的验证集严格的就是你输入数据最末的x%。</p>
<hr />
<p><a name='shuffle'>
<font color='#404040'></p>
<h2 id="_4">训练数据在训练时会被随机洗乱吗？</h2>
<p></font>
</a></p>
<p>是的，如果<code>model.fit</code>的<code>shuffle</code>参数为真，训练的数据就会被随机洗乱。不设置时默认为真。训练数据会在每个epoch的训练中都重新洗乱一次。</p>
<p>验证集的数据不会被洗乱</p>
<hr />
<p><a name='history'>
<font color='#404040'></p>
<h2 id="epochloss">如何在每个epoch后记录训练/测试的loss和正确率？</h2>
<p></font>
</a></p>
<p><code>model.fit</code>在运行结束后返回一个<code>History</code>对象，其中含有的<code>history</code>属性包含了训练过程中损失函数的值以及其他度量指标。</p>
<pre><code class="python">hist = model.fit(X, y, validation_split=0.2)
print(hist.history)
</code></pre>

<hr />
<p><a name='statful_RNN'>
<font color='#404040'></p>
<h2 id="rnnstatful-rnn">如何使用状态RNN（statful RNN）？</h2>
<p></font>
</a></p>
<p>一个RNN是状态RNN，意味着训练时每个batch的状态都会被重用于初始化下一个batch的初始状态。</p>
<p>当使用状态RNN时，有如下假设</p>
<ul>
<li>
<p>所有的batch都具有相同数目的样本</p>
</li>
<li>
<p>如果<code>X1</code>和<code>X2</code>是两个相邻的batch，那么对于任何<code>i</code>，<code>X2[i]</code>都是<code>X1[i]</code>的后续序列</p>
</li>
</ul>
<p>要使用状态RNN，我们需要</p>
<ul>
<li>
<p>显式的指定每个batch的大小。可以通过模型的首层参数<code>batch_input_shape</code>来完成。<code>batch_input_shape</code>是一个整数tuple，例如(32,10,16)代表一个具有10个时间步，每步向量长为16，每32个样本构成一个batch的输入数据格式。</p>
</li>
<li>
<p>在RNN层中，设置<code>stateful=True</code></p>
</li>
</ul>
<p>要重置网络的状态，使用：</p>
<ul>
<li>
<p><code>model.reset_states()</code>来重置网络中所有层的状态</p>
</li>
<li>
<p><code>layer.reset_states()</code>来重置指定层的状态</p>
</li>
</ul>
<p>例子：</p>
<pre><code class="python">X  # this is our input data, of shape (32, 21, 16)
# we will feed it to our model in sequences of length 10

model = Sequential()
model.add(LSTM(32, batch_input_shape=(32, 10, 16), stateful=True))
model.add(Dense(16, activation='softmax'))

model.compile(optimizer='rmsprop', loss='categorical_crossentropy')

# we train the network to predict the 11th timestep given the first 10:
model.train_on_batch(X[:, :10, :], np.reshape(X[:, 10, :], (32, 16)))

# the state of the network has changed. We can feed the follow-up sequences:
model.train_on_batch(X[:, 10:20, :], np.reshape(X[:, 20, :], (32, 16)))

# let's reset the states of the LSTM layer:
model.reset_states()

# another way to do it in this case:
model.layers[0].reset_states()
</code></pre>

<p>注意，<code>predict</code>，<code>fit</code>，<code>train_on_batch</code>
，<code>predict_classes</code>等方法都会更新模型中状态层的状态。这使得你可以不但可以进行状态网络的训练，也可以进行状态网络的预测。</p>
<hr />
<p><a name='multi-GPU'>
<font color='#404040'></p>
<h2 id="kerasgpu_1">如何使用Keras进行分布式/多GPU运算？</h2>
<p></font>
</a></p>
<p>Keras在使用TensorFlow作为后端的时候可以进行分布式/多GPU的运算，Keras对多GPU和分布式的支持是通过TF完成的。</p>
<pre><code class="python">with tf.device('/gpu:0'):
    x = tf.placeholder(tf.float32, shape=(None, 20, 64))
    y = LSTM(32)(x)  # all ops in the LSTM layer will live on GPU:0

with tf.device('/gpu:1'):
    x = tf.placeholder(tf.float32, shape=(None, 20, 64))
    y = LSTM(32)(x)  # all ops in the LSTM layer will live on GPU:1
</code></pre>

<p>注意，上例中由LSTM创建的变量不在GPU上：所有的TensorFlow变量总是在CPU上生存，而与它们在哪创建无关。各个设备上的变量转换TensorFlow会自动完成。</p>
<p>如果你想在不同的GPU上训练同一个模型的不同副本，但在不同的副本中共享权重，你应该首先在一个设备上实例化你的模型，然后在不同的设备上多次调用该对象，例如：</p>
<pre><code class="python">with tf.device('/cpu:0'):
    x = tf.placeholder(tf.float32, shape=(None, 784))

    # shared model living on CPU:0
    # it won't actually be run during training; it acts as an op template
    # and as a repository for shared variables
    model = Sequential()
    model.add(Dense(32, activation='relu', input_dim=784))
    model.add(Dense(10, activation='softmax'))

# replica 0
with tf.device('/gpu:0'):
    output_0 = model(x)  # all ops in the replica will live on GPU:0

# replica 1
with tf.device('/gpu:1'):
    output_1 = model(x)  # all ops in the replica will live on GPU:1

# merge outputs on CPU
with tf.device('/cpu:0'):
    preds = 0.5 * (output_0 + output_1)

# we only run the `preds` tensor, so that only the two
# replicas on GPU get run (plus the merge op on CPU)
output_value = sess.run([preds], feed_dict={x: data})

</code></pre>

<p>要想完成分布式的训练，你需要将Keras注册在连接一个集群的TensorFlow会话上：</p>
<pre><code class="python">server = tf.train.Server.create_local_server()
sess = tf.Session(server.target)

from keras import backend as K
K.set_session(sess)
</code></pre>

<p>关于分布式训练的更多信息，请参考<a href="https://www.tensorflow.org/versions/r0.8/how_tos/distributed/index.html"><font color="#FF0000">这里</font></a></p>
<hr />
<p><a name='freeze'>
<font color='#404040'></p>
<h2 id="_5">如何“冻结”网络的层？</h2>
<p></font>
</a></p>
<p>“冻结”一个层指的是该层将不参加网络训练，即该层的权重永不会更新。在进行fine-tune时我们经常会需要这项操作。
在使用固定的embedding层处理文本输入时，也需要这个技术。</p>
<p>可以通过向层的构造函数传递<code>trainable</code>参数来指定一个层是不是可训练的，如：</p>
<pre><code class="python">frozen_layer = Dense(32,trainable=False)
</code></pre>

<p>此外，也可以通过将层对象的<code>trainable</code>属性设为<code>True</code>或<code>False</code>来为已经搭建好的模型设置要冻结的层。
在设置完后，需要运行<code>compile</code>来使设置生效，例如：</p>
<pre><code class="python">x = Input(shape=(32,))
layer = Dense(32)
layer.trainable = False
y = layer(x)

frozen_model = Model(x, y)
# in the model below, the weights of `layer` will not be updated during training
frozen_model.compile(optimizer='rmsprop', loss='mse')

layer.trainable = True
trainable_model = Model(x, y)
# with this model the weights of the layer will be updated during training
# (which will also affect the above model since it uses the same layer instance)
trainable_model.compile(optimizer='rmsprop', loss='mse')

frozen_model.fit(data, labels)  # this does NOT update the weights of `layer`
trainable_model.fit(data, labels)  # this updates the weights of `layer`
</code></pre>

<hr />
<p><a name='pop'>
<font color='#404040'></p>
<h2 id="sequential">如何从Sequential模型中去除一个层？</h2>
<p></font>
</a></p>
<p>可以通过调用<code>.pop()</code>来去除模型的最后一个层，反复调用n次即可去除模型后面的n个层</p>
<pre><code class="python">model = Sequential()
model.add(Dense(32, activation='relu', input_dim=784))
model.add(Dense(32, activation='relu'))

print(len(model.layers))  # &quot;2&quot;

model.pop()
print(len(model.layers))  # &quot;1&quot;
</code></pre>

<p>【Tips】模型的.layers属性保存了模型中的层对象，数据类型是list，在model没有<code>.pop()</code>方法前，我一般通过model.layers.pop()完成相同的功能。
但显然，使用keras提供的方法会安全的多【@bigmoyan】</p>
<hr />
<p><a name='pretrain'>
<font color='#404040'></p>
<h2 id="keras_3">如何在Keras中使用预训练的模型？</h2>
<p></font>
</a></p>
<p>我们提供了下面这些图像分类的模型代码及预训练权重：</p>
<ul>
<li>VGG16</li>
<li>VGG19</li>
<li>ResNet50</li>
<li>Inception v3</li>
</ul>
<p>可通过<code>keras.applications</code>载入这些模型：</p>
<pre><code class="python">from keras.applications.vgg16 import VGG16
from keras.applications.vgg19 import VGG19
from keras.applications.resnet50 import ResNet50
from keras.applications.inception_v3 import InceptionV3

model = VGG16(weights='imagenet', include_top=True)
</code></pre>

<p>这些代码的使用示例请参考<code>.Application</code>模型的<a href="../../other/application/"><font color='#FF0000'>文档</font></a></p>
<p>使用这些预训练模型进行特征抽取或fine-tune的例子可以参考<a href="http://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"><font color='#FF0000'>此博客</font></a></p>
<p>VGG模型也是很多Keras例子的基础模型，如：</p>
<ul>
<li><a href="https://github.com/fchollet/keras/blob/master/examples/neural_style_transfer.py"><font color='#FF0000'>Style-transfer</font></a></li>
<li><a href="https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py"><font color='#FF0000'>Feature visualization</font></a></li>
<li><a href="https://github.com/fchollet/keras/blob/master/examples/deep_dream.py"><font color='#FF0000'>Deep dream</font></a></li>
</ul>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../trap/" class="btn btn-neutral float-right" title="Keras使用陷阱">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../functional_API/" class="btn btn-neutral" title="快速开始泛型模型"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../functional_API/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../trap/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../../js/theme.js"></script>

</body>
</html>
